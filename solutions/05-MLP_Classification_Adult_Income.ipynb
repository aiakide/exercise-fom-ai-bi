{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook: Klassifikation mit einem Multi Layer Perceptron in PyTorch\n",
    "\n",
    "In diesem Notebook werden wir ein **Multi Layer Perceptron (MLP)** mit **PyTorch** erstellen, um eine Klassifikationsaufgabe auf tabellarischen Daten durchzuführen. Wir verwenden den **Adult Income Dataset**, der demografische Informationen enthält und die Aufgabe stellt, vorherzusagen, ob eine Person ein Einkommen über oder unter 50.000 USD hat. Dies ist eine binäre Klassifikationsaufgabe.\n",
    "\n",
    "Das Notebook ist in folgende Abschnitte unterteilt:\n",
    "1. **Datenvorbereitung**: Laden, Bereinigen und Vorbereiten des Datensatzes.\n",
    "2. **Erstellen von PyTorch Datasets und Dataloaders**: Umwandeln der Daten in ein für PyTorch geeignetes Format.\n",
    "3. **Definition des MLP-Modells**: Erstellen eines einfachen MLP mit einer versteckten Schicht.\n",
    "4. **Training des Modells**: Verwenden des Adam-Optimizers und der Cross-Entropy-Loss-Funktion.\n",
    "5. **Evaluation des Modells**: Berechnen von Metriken wie Accuracy, Precision, Recall und F1-Score sowie Visualisierung der Confusion Matrix.\n",
    "\n",
    "Jeder Abschnitt enthält Erklärungen und Code, um den Studierenden ein tiefes Verständnis der Konzepte zu vermitteln."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datenvorbereitung\n",
    "\n",
    "Zuerst müssen wir den Datensatz laden und vorbereiten. Der **Adult Income Dataset** enthält sowohl numerische als auch kategorische Merkmale, die wir verarbeiten müssen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Datensatz laden\n",
    "\n",
    "Der Datensatz kann von [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/adult) heruntergeladen werden. Für dieses Beispiel laden wir ihn direkt aus dem Internet."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# URL des Datensatzes\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "\n",
    "# Spaltennamen\n",
    "columns = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
    "    \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
    "    \"hours-per-week\", \"native-country\", \"income\"\n",
    "]\n",
    "\n",
    "# Daten laden\n",
    "data = pd.read_csv(url, header=None, names=columns, na_values=\" ?\", skipinitialspace=True)\n",
    "\n",
    "# Erste Zeilen anzeigen\n",
    "print(data.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Daten bereinigen\n",
    "\n",
    "Wir entfernen Zeilen mit fehlenden Werten und kodieren kategorische Variablen in numerische Werte."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fehlende Werte entfernen\n",
    "data = data.dropna()\n",
    "\n",
    "# Kategorische Variablen in numerische umwandeln\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_cols = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\", \"income\"]\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Zielvariable (income) extrahieren\n",
    "X = data.drop(\"income\", axis=1)\n",
    "y = data[\"income\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Daten in Trainings-, Validierungs- und Testsets aufteilen\n",
    "\n",
    "Wir teilen den Datensatz in 70% Training, 15% Validierung und 15% Test."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Aufteilung in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Aufteilung des Trainingsdatensatzes in Trainings- und Validierungsdaten\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1765, random_state=42)  # 0.1765 * 85% ≈ 15%"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Daten skalieren\n",
    "\n",
    "Da neuronale Netze empfindlich auf die Skalierung der Eingabedaten reagieren, standardisieren wir die numerischen Merkmale."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Erstellen von PyTorch Datasets und Dataloaders\n",
    "\n",
    "PyTorch arbeitet mit **Datasets** und **Dataloaders**, um Daten effizient zu laden und zu verarbeiten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Benutzerdefiniertes Dataset erstellen\n",
    "\n",
    "Wir erstellen ein benutzerdefiniertes Dataset, das die Daten in Tensoren umwandelt."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AdultDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.long)  # Annahme: y ist ein pandas Series\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Datasets erstellen\n",
    "train_dataset = AdultDataset(X_train, y_train)\n",
    "val_dataset = AdultDataset(X_val, y_val)\n",
    "test_dataset = AdultDataset(X_test, y_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataloaders erstellen\n",
    "\n",
    "Dataloaders ermöglichen das Laden der Daten in Batches und das Mischen der Daten."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "batch_size = 64\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Dataloaders mit pin_memory=True, wenn CUDA verfügbar ist\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=device != \"cpu\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=device != \"cpu\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=device != \"cpu\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Definition des MLP-Modells\n",
    "\n",
    "Ein **Multi Layer Perceptron (MLP)** besteht aus einer Eingabeschicht, einer oder mehreren versteckten Schichten und einer Ausgabeschicht. Für dieses Beispiel verwenden wir ein einfaches MLP mit einer versteckten Schicht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Modellarchitektur\n",
    "\n",
    "- **Eingabeschicht**: Die Anzahl der Neuronen entspricht der Anzahl der Merkmale (Features) im Datensatz.\n",
    "- **Versteckte Schicht**: Wir wählen 64 Neuronen mit ReLU-Aktivierungsfunktion.\n",
    "- **Ausgabeschicht**: 2 Neuronen (für die binäre Klassifikation) mit Softmax-Aktivierungsfunktion."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Parameter\n",
    "input_size = X_train.shape[1]  # Anzahl der Features\n",
    "hidden_size = 128\n",
    "num_classes = 2  # Binäre Klassifikation\n",
    "\n",
    "# Modell instanziieren\n",
    "model = MLP(input_size, hidden_size, num_classes)\n",
    "\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training des Modells\n",
    "\n",
    "Wir trainieren das Modell mit dem **Adam-Optimizer** und der **Cross-Entropy-Loss-Funktion**, die für Klassifikationsaufgaben geeignet ist."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.1 Funktion zum Plotten der Verluste\n",
    "\n",
    "Wir definieren eine Funktion, die die Trainings- und Validierungsverluste über die Epochen hinweg speichert und einen Plot erstellt."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(train_losses, val_losses, num_epochs):\n",
    "    \"\"\"\n",
    "    Plottet die Trainings- und Validierungsverluste über die Epochen.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list): Liste der durchschnittlichen Trainingsverluste pro Epoche\n",
    "        val_losses (list): Liste der durchschnittlichen Validierungsverluste pro Epoche\n",
    "        num_epochs (int): Anzahl der Epochen\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Trainings- und Validierungsverluste über Epochen')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2 Trainingsschleife\n",
    "\n",
    "Wir trainieren das Modell für eine bestimmte Anzahl von Epochen und überwachen die Leistung auf dem Validierungsdatensatz."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss-Funktion und Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.007)\n",
    "\n",
    "# Trainingsparameter\n",
    "num_epochs = 22\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Daten auf das Gerät verschieben\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward Pass und Optimierung\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Durchschnittlichen Trainingsverlust speichern\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validierung\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Durchschnittlichen Validierungsverlust speichern\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Verluste plotten\n",
    "plot_losses(train_losses, val_losses, num_epochs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation des Modells\n",
    "\n",
    "Nach dem Training evaluieren wir das Modell auf dem Testdatensatz mit gängigen Metriken wie **Accuracy**, **Precision**, **Recall** und **F1-Score**. Zusätzlich visualisieren wir die **Confusion Matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Metriken berechnen\n",
    "\n",
    "Wir verwenden die Funktionen von `sklearn` zur Berechnung der Metriken."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Modell auf Testdatensatz evaluieren\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Metriken berechnen\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Confusion Matrix visualisieren\n",
    "\n",
    "Die Confusion Matrix hilft uns, die Vorhersagen des Modells besser zu verstehen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Confusion Matrix berechnen\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plotten\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"<=50K\", \">50K\"], yticklabels=[\"<=50K\", \">50K\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Notebook haben wir ein Multi Layer Perceptron (MLP) mit PyTorch erstellt, um eine binäre Klassifikationsaufgabe auf dem Adult Income Dataset durchzuführen. Wir haben den Datensatz vorbereitet, PyTorch Datasets und Dataloaders erstellt, das MLP-Modell definiert und trainiert, und schließlich das Modell mit gängigen Metriken evaluiert. Die Confusion Matrix bietet eine visuelle Darstellung der Modellleistung.\n",
    "\n",
    "Dieses Beispiel zeigt, wie Deep Learning auch auf tabellarischen Daten angewendet werden kann und dient als Einführung in die Verwendung von PyTorch für solche Aufgaben.\n",
    "\n",
    "---\n",
    "\n",
    "**Hinweis**: Stellen Sie sicher, dass alle erforderlichen Bibliotheken installiert sind, indem Sie `pip install pandas torch scikit-learn seaborn matplotlib` ausführen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
