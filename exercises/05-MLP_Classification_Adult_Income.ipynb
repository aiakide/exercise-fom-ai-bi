{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook: Klassifikation mit einem Multi Layer Perceptron in PyTorch\n",
    "\n",
    "In diesem Notebook werden wir ein **Multi Layer Perceptron (MLP)** mit **PyTorch** erstellen, um eine Klassifikationsaufgabe auf tabellarischen Daten durchzuführen. Wir verwenden den **Adult Income Dataset**, der demografische Informationen enthält und die Aufgabe stellt, vorherzusagen, ob eine Person ein Einkommen über oder unter 50.000 USD hat. Dies ist eine binäre Klassifikationsaufgabe.\n",
    "\n",
    "Das Notebook ist in folgende Abschnitte unterteilt:\n",
    "1. **Datenvorbereitung**: Laden, Bereinigen und Vorbereiten des Datensatzes.\n",
    "2. **Erstellen von PyTorch Datasets und Dataloaders**: Umwandeln der Daten in ein für PyTorch geeignetes Format.\n",
    "3. **Definition des MLP-Modells**: Erstellen eines einfachen MLP mit einer versteckten Schicht.\n",
    "4. **Training des Modells**: Verwenden des Adam-Optimizers und der Cross-Entropy-Loss-Funktion.\n",
    "5. **Evaluation des Modells**: Berechnen von Metriken wie Accuracy, Precision, Recall und F1-Score sowie Visualisierung der Confusion Matrix.\n",
    "\n",
    "Jeder Abschnitt enthält Erklärungen und Code, um den Studierenden ein tiefes Verständnis der Konzepte zu vermitteln."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datenvorbereitung\n",
    "\n",
    "Zuerst müssen wir den Datensatz laden und vorbereiten. Der **Adult Income Dataset** enthält sowohl numerische als auch kategorische Merkmale, die wir verarbeiten müssen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Datensatz laden\n",
    "\n",
    "Der Datensatz kann von [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/adult) heruntergeladen werden. Für dieses Beispiel laden wir ihn direkt aus dem Internet."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# URL des Datensatzes\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "\n",
    "# Spaltennamen\n",
    "columns = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
    "    \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
    "    \"hours-per-week\", \"native-country\", \"income\"\n",
    "]\n",
    "\n",
    "# Daten laden\n",
    "data = pd.read_csv(url, header=None, names=columns, na_values=\" ?\", skipinitialspace=True)\n",
    "\n",
    "# Erste Zeilen anzeigen\n",
    "print(data.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Daten bereinigen\n",
    "\n",
    "Wir entfernen Zeilen mit fehlenden Werten und kodieren kategorische Variablen in numerische Werte."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fehlende Werte entfernen\n",
    "data = data.dropna()\n",
    "\n",
    "# Kategorische Variablen in numerische umwandeln\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_cols = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\", \"income\"]\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Zielvariable (income) extrahieren\n",
    "X = data.drop(\"income\", axis=1)\n",
    "y = data[\"income\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Daten in Trainings-, Validierungs- und Testsets aufteilen\n",
    "\n",
    "Wir teilen den Datensatz in 70% Training, 15% Validierung und 15% Test."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Aufteilung in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Aufteilung des Trainingsdatensatzes in Trainings- und Validierungsdaten\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1765, random_state=42)  # 0.1765 * 85% ≈ 15%"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Daten skalieren\n",
    "\n",
    "Da neuronale Netze empfindlich auf die Skalierung der Eingabedaten reagieren, standardisieren wir die numerischen Merkmale."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Erstellen von PyTorch Datasets und Dataloaders\n",
    "\n",
    "PyTorch arbeitet mit **Datasets** und **Dataloaders**, um Daten effizient zu laden und zu verarbeiten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Benutzerdefiniertes Dataset erstellen\n",
    "\n",
    "Wir erstellen ein benutzerdefiniertes Dataset, das die Daten in Tensoren umwandelt."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataloaders erstellen\n",
    "\n",
    "Dataloaders ermöglichen das Laden der Daten in Batches und das Mischen der Daten."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Definition des MLP-Modells\n",
    "\n",
    "Ein **Multi Layer Perceptron (MLP)** besteht aus einer Eingabeschicht, einer oder mehreren versteckten Schichten und einer Ausgabeschicht. Für dieses Beispiel verwenden wir ein einfaches MLP mit einer versteckten Schicht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Modellarchitektur\n",
    "\n",
    "- **Eingabeschicht**: Die Anzahl der Neuronen entspricht der Anzahl der Merkmale (Features) im Datensatz.\n",
    "- **Versteckte Schicht**: Wir wählen 64 Neuronen mit ReLU-Aktivierungsfunktion.\n",
    "- **Ausgabeschicht**: 2 Neuronen (für die binäre Klassifikation) mit Softmax-Aktivierungsfunktion."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training des Modells\n",
    "\n",
    "Wir trainieren das Modell mit dem **Adam-Optimizer** und der **Cross-Entropy-Loss-Funktion**, die für Klassifikationsaufgaben geeignet ist."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.1 Funktion zum Plotten der Verluste\n",
    "\n",
    "Wir definieren eine Funktion, die die Trainings- und Validierungsverluste über die Epochen hinweg speichert und einen Plot erstellt."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(train_losses, val_losses, num_epochs):\n",
    "    \"\"\"\n",
    "    Plottet die Trainings- und Validierungsverluste über die Epochen.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list): Liste der durchschnittlichen Trainingsverluste pro Epoche\n",
    "        val_losses (list): Liste der durchschnittlichen Validierungsverluste pro Epoche\n",
    "        num_epochs (int): Anzahl der Epochen\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Trainings- und Validierungsverluste über Epochen')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2 Trainingsschleife\n",
    "\n",
    "Wir trainieren das Modell für eine bestimmte Anzahl von Epochen und überwachen die Leistung auf dem Validierungsdatensatz."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation des Modells\n",
    "\n",
    "Nach dem Training evaluieren wir das Modell auf dem Testdatensatz mit gängigen Metriken wie **Accuracy**, **Precision**, **Recall** und **F1-Score**. Zusätzlich visualisieren wir die **Confusion Matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Metriken berechnen\n",
    "\n",
    "Wir verwenden die Funktionen von `sklearn` zur Berechnung der Metriken."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Confusion Matrix visualisieren\n",
    "\n",
    "Die Confusion Matrix hilft uns, die Vorhersagen des Modells besser zu verstehen."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Confusion Matrix berechnen\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plotten\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"<=50K\", \">50K\"], yticklabels=[\"<=50K\", \">50K\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
