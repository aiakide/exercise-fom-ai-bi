{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Jupyter Notebook: Einfaches CNN und AlexNet Implementierung mit CIFAR-10\n",
    "\n",
    "In diesem Notebook lernst du, wie du ein einfaches Convolutional Neural Network (CNN) auf dem CIFAR-10-Datensatz trainierst. Du wirst die Trainings- und Validierungsverluste nach jeder Epoche plotten und eine Heatmap-Funktion auf ein Beispielbild aus dem Validierungsdatensatz anwenden, um aktivierte Bildregionen zu visualisieren. Im zweiten Teil wirst du angeleitet, AlexNet anhand eines Bildes der Architektur selbst zu implementieren."
   ],
   "id": "47f789c2959a7cad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Datensatz vorbereiten\n",
    "\n",
    "Du wirst den CIFAR-10-Datensatz verwenden, der 60.000 32x32-Farbbilder in 10 Klassen enthält. Der Trainingsdatensatz wird in 80% Training und 20% Validierung aufgeteilt."
   ],
   "id": "75bd280b18ae30db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Transformationen für die Bilder\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Trainingsdatensatz laden\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# In Trainings- und Validierungssatz aufteilen (80% Training, 20% Validierung)\n",
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "train_subset, val_subset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Testdatensatz laden\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Klassen\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "id": "5865ce2c386e9f43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Einfaches CNN definieren\n",
    "\n",
    "Definiere ein einfaches CNN mit zwei Convolutional Layers und drei Fully Connected Layers."
   ],
   "id": "a15dbde29a173c23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Modell initialisieren und auf GPU verschieben, falls verfügbar\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "model = SimpleCNN().to(device)"
   ],
   "id": "5197064aeba268bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Training des Modells\n",
    "\n",
    "Trainiere das Modell mit dem Adam-Optimizer und der Cross-Entropy Loss-Funktion. Nach dem Training werden die Verluste geplottet."
   ],
   "id": "afeb6fd8221e89d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Verlustfunktion und Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ],
   "id": "bd580aae07f2ed77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Val Loss', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Trainings- und Validierungsverluste')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "5a1186b1dfeec1ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Trainingsschleife\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(trainloader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss /= len(valloader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracy = correct / total\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "plot_losses(train_losses, val_losses)"
   ],
   "id": "49e5254cabfd0439",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Evaluation auf Testdatensatz\n",
    "\n",
    "Evaluiere das Modell auf dem Testdatensatz, um die endgültige Leistung zu überprüfen."
   ],
   "id": "50bd9ca4fcbfa007"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "test_loss /= len(testloader)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Test Loss: {val_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n"
   ],
   "id": "b1f87768e209f096",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. AlexNet implementieren\n",
    "\n",
    "In diesem Abschnitt wirst du AlexNet anhand eines Bildes der Architektur implementieren. Deine Aufgabe ist es, die Architektur in PyTorch zu programmieren, indem du die Schichten und Verbindungen aus dem Bild nachbaust.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/AlexNet_block_diagram.svg/1920px-AlexNet_block_diagram.svg.png)"
   ],
   "id": "1099efb966d5cb88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "### Anleitung zur Implementierung\n",
    "\n",
    "Um AlexNet zu implementieren, folge diesen Schritten:\n",
    "**1. Analysiere das Bild**:\n",
    "_ Schaue dir die Schichten der Architektur genau an. Beachte die Anzahl der Convolutional Layers, Pooling Layers und Fully Connected Layers.\n",
    "- Identifiziere die Anzahl der Filter, Kernel-Größen, Strides, Padding-Werte und Aktivierungsfunktionen (z.B. ReLU).\n",
    "- Achte auf Dropout-Schichten und die Größe der Fully Connected Layers.\n",
    "\n",
    "**2. Erstelle das Modell in PyTorch**:\n",
    "- Definiere eine Klasse AlexNet, die von nn.Module erbt\n",
    "- Implementiere die Convolutional Layers mit nn.Conv2d, Pooling Layers mit nn.MaxPool2d und Fully Connected Layers mit nn.Linear.\n",
    "- Füge ReLU-Aktivierungen (F.relu oder nn.ReLU) und Dropout (nn.Dropout) hinzu, wie im Bild angegeben.\n",
    "- Stelle sicher, dass die Eingabegröße (32x32 für CIFAR-10) mit der Architektur kompatibel ist. Du musst möglicherweise Kernel-Größen oder Strides anpassen, da AlexNet ursprünglich für 224x224-Bilder entwickelt wurde.\n",
    "\n",
    "**3. Forward-Pass**:\n",
    "- Implementiere die forward-Methode, die die Eingabe durch die Schichten führt.\n",
    "- Achte darauf, die Dimensionen nach den Convolutional Layers zu flatten (z.B. mit x.view), bevor du die Fully Connected Layers anwendest.\n",
    "\n",
    "**4. Training**:\n",
    "- Trainiere dein AlexNet-Modell ähnlich wie das einfache CNN oben. Verwende den gleichen trainloader, valloader und testloader, die Adam-Optimizer und Cross-Entropy Loss.\n",
    "- Du kannst die gleichen Funktionen (validate_model, plot_losses, create_heatmap) wiederverwenden, um Verluste zu plotten und Heatmaps zu erstellen.\n",
    "\n",
    "### Tipps\n",
    "- **Dimensionen überprüfen**: Berechne die Ausgabegrößen jeder Schicht, um sicherzustellen, dass die Fully Connected Layers die richtige Eingabegröße erhalten. Für CIFAR-10 (32x32) musst du die Parameter anpassen, da AlexNet für größere Bilder ausgelegt ist.\n",
    "- **Debugging**: Teste dein Modell mit einem einzelnen Batch, um sicherzustellen, dass die Dimensionen korrekt sind (z.B. output = model(torch.randn(1, 3, 32, 32).to(device))).\n",
    "- **Anpassungen**: Falls die ursprüngliche AlexNet-Architektur zu komplex ist, kannst du die Anzahl der Filter oder Schichten reduzieren, um sie für CIFAR-10 zu optimieren.\n",
    "\n",
    "### Beispiel für den Anfang\n",
    "\n",
    "Hier ist ein Gerüst, um dir den Einstieg zu erleichtern, ohne die vollständige Implementierung zu verraten:"
   ],
   "id": "bdd493f26e2697e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv1: 3 input channels (RGB), 64 filters, kernel 3x3, stride 1, padding 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 64x16x16\n",
    "\n",
    "            # Conv2: 64 input channels, 192 filters, kernel 3x3, padding 1\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 192x8x8\n",
    "\n",
    "            # Conv3: 192 input channels, 384 filters, kernel 3x3, padding 1\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Conv4: 384 input channels, 256 filters, kernel 3x3, padding 1\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Conv5: 256 input channels, 256 filters, kernel 3x3, padding 1\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 256x4x4\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256 * 4 * 4, 1024),  # Angepasst an 4x4 Feature Maps\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 4 * 4)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Modell initialisieren und auf GPU verschieben\n",
    "alexnet = AlexNet().to(device)"
   ],
   "id": "12dcc31e6949f45d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Aufgabe: Fülle die features und classifier Abschnitte basierend auf dem Bild der Architektur aus. Trainiere das Modell anschließend mit der gleichen Trainingsschleife wie beim einfachen CNN und überprüfe die Leistung auf dem Testdatensatz.",
   "id": "4913b18e89809e00"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.2 Erklärung der Anpassungen\n",
    "\n",
    "Die Original-AlexNet-Architektur (für 224x224-Bilder) wurde wie folgt an CIFAR-10 (32x32-Bilder) angepasst:\n",
    "\n",
    "- **Kernel-Größen und Strides**: Kleinere Kernel (3x3 statt 11x11) und Strides (1 statt 4) für Conv1, um die kleineren Eingabebilder zu berücksichtigen.\n",
    "- **Pooling**: Max-Pooling-Schichten wurden beibehalten, aber mit kleineren Kerneln (2x2 statt 3x3), um die Feature Maps schrittweise zu reduzieren.\n",
    "- **Fully Connected Layers**: Die Anzahl der Neuronen wurde auf 1024 reduziert (statt 4096), und die Eingabegröße der ersten FC-Schicht wurde an die Ausgabe der letzten Convolutional Layer (256x4x4) angepasst.\n",
    "- **Dropout**: Dropout (p=0.5) wurde beibehalten, um Overfitting zu verhindern.\n",
    "- **Dimensionen**: Die Architektur wurde so gestaltet, dass die Feature Maps von 32x32 bis 4x4 reduziert werden, kompatibel mit CIFAR-10."
   ],
   "id": "227709d950d0b00e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Optimizer für AlexNet\n",
    "optimizer_alexnet = optim.Adam(alexnet.parameters(), lr=0.001)\n",
    "\n",
    "# Trainingsschleife für AlexNet\n",
    "train_losses_alexnet = []\n",
    "val_losses_alexnet = []\n",
    "val_accuracies_alexnet = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    alexnet.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer_alexnet.zero_grad()\n",
    "        outputs = alexnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_alexnet.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(trainloader)\n",
    "    train_losses_alexnet.append(train_loss)\n",
    "\n",
    "    alexnet.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = alexnet(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss /= len(valloader)\n",
    "    val_losses_alexnet.append(val_loss)\n",
    "    val_accuracy = correct / total\n",
    "\n",
    "    val_accuracies_alexnet.append(val_accuracy)\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "plot_losses(train_losses_alexnet, val_losses_alexnet)"
   ],
   "id": "521d87be6a30acbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "alexnet.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = alexnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "test_loss /= len(testloader)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Test Loss: {val_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n"
   ],
   "id": "9f34c67634f2e107"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
