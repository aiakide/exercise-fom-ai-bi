{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Jupyter Notebook: Regression mit einem Multi Layer Perceptron in PyTorch\n",
    "\n",
    "In diesem Notebook erstellen wir ein Multi Layer Perceptron (MLP) mit PyTorch, um eine Regressionsaufgabe auf dem California Housing Dataset zu lösen. Der Datensatz enthält Merkmale wie Medianalter des Hauses, Anzahl der Zimmer und Einkommen der Nachbarschaft, um den Medianhauspreis (kontinuierlich) vorherzusagen. Wir integrieren CUDA-Unterstützung für GPU-Beschleunigung, falls verfügbar, und visualisieren die Trainings- und Validierungsverluste. Am Ende evaluieren wir das Modell mit Metriken wie Mean Squared Error (MSE), Mean Absolute Error (MAE) und R² Score und erstellen einen Scatter-Plot der vorhergesagten vs. tatsächlichen Werte.\n",
    "\n",
    "Das Notebook ist in folgende Abschnitte gegliedert:\n",
    "1. Datenvorbereitung: Laden und Skalieren des Datensatzes.\n",
    "2. PyTorch Datasets und Dataloaders: Daten für PyTorch vorbereiten.\n",
    "3. MLP-Modell: Definition eines einfachen MLP für Regression.\n",
    "4. Training: Training mit MSE Loss und Visualisierung der Verluste.\n",
    "5. Evaluation: Berechnung von Metriken und Visualisierung der Ergebnisse."
   ],
   "id": "1155ca3c6db98dc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Datenvorbereitung\n",
    "\n",
    "Wir laden den California Housing Dataset und skalieren die Daten, um die Modellleistung zu optimieren.\n",
    "\n",
    "## 1.1 Datensatz laden"
   ],
   "id": "a0cbbbc6ddac2f57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "# Datensatz laden\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target  # Zielvariable: Medianhauspreis in 100.000 USD\n",
    "\n",
    "# Erste Zeilen anzeigen\n",
    "print(X.head())\n",
    "print(f\"Zielvariable (erste 5 Werte): {y[:5]}\")"
   ],
   "id": "fba9ecbf5641d417",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 Daten aufteilen\n",
    "\n",
    "Wir teilen die Daten in Trainings- (70%), Validierungs- (15%) und Testsets (15%)."
   ],
   "id": "9f409edd4c319525"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Aufteilung in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Aufteilung des Trainingsdatensatzes in Trainings- und Validierungsdaten\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1765, random_state=42)  # 0.1765 * 85% ≈ 15%"
   ],
   "id": "bf56955ae7d50a39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.3 Daten skalieren\n",
    "\n",
    "Wir standardisieren die Merkmale und die Zielvariable mit StandardScaler."
   ],
   "id": "9e5bbfcac8c55b59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Skalierung der Merkmale\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_val = scaler_X.transform(X_val)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "# Skalierung der Zielvariable\n",
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_val = scaler_y.transform(y_val.reshape(-1, 1)).flatten()\n",
    "y_test = scaler_y.transform(y_test.reshape(-1, 1)).flatten()"
   ],
   "id": "f9c8ffcc71d32e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Erstellen von PyTorch Datasets und Dataloaders\n",
    "\n",
    "Wir wandeln die Daten in PyTorch-Tensoren um und erstellen Dataloaders für effizientes Training.\n",
    "\n",
    "### 2.1 Benutzerdefiniertes Dataset"
   ],
   "id": "91dbf7d60239d9b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4ca02dd241791bb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.2 Dataloaders\n",
    "\n",
    "Wir nutzen pin_memory=True für schnellere Datenübertragung zur GPU, falls CUDA verfügbar ist."
   ],
   "id": "830b300edaefaf32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "83edc08324b6d4b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Definition des MLP-Modells\n",
    "\n",
    "Unser MLP hat eine Eingabeschicht, eine versteckte Schicht mit ReLU-Aktivierung und eine Ausgabeschicht für Regression."
   ],
   "id": "a9576a211dcd98f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fb98190c356c0b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.1 Funktion zum Plotten der Verluste\n",
    "\n",
    "Wir definieren eine Funktion, die die Trainings- und Validierungsverluste über die Epochen hinweg speichert und einen Plot erstellt."
   ],
   "id": "b3a15c4bb8c73b51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(train_losses, val_losses, num_epochs):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Trainings- und Validierungsverluste über Epochen')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "a823125aa08dc8af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2 Trainingsschleife",
   "id": "8e93c4e3b67cd7d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "be9b270f1e152993",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Evaluation des Modells\n",
    "\n",
    "Wir evaluieren das Modell auf dem Testset mit MSE, MAE und R² Score und visualisieren die Ergebnisse.\n",
    "\n",
    "### 5.1 Metriken berechnen"
   ],
   "id": "a9171b9f40421b17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "942976b4bbd69e6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.2 Scatter-Plot der Ergebnisse",
   "id": "f0cdcf0928f2b78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Scatter-Plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(all_labels, all_preds, alpha=0.5)\n",
    "plt.plot([all_labels.min(), all_labels.max()], [all_labels.min(), all_labels.max()], 'r--', lw=2)\n",
    "plt.xlabel('Tatsächliche Hauspreise (in 100.000 USD)')\n",
    "plt.ylabel('Vorhergesagte Hauspreise (in 100.000 USD)')\n",
    "plt.title('Vorhergesagte vs. Tatsächliche Hauspreise')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "374d473f14f5e5bf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
