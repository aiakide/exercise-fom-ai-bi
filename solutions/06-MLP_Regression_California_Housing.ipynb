{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Jupyter Notebook: Regression mit einem Multi Layer Perceptron in PyTorch\n",
    "\n",
    "In diesem Notebook erstellen wir ein Multi Layer Perceptron (MLP) mit PyTorch, um eine Regressionsaufgabe auf dem California Housing Dataset zu lösen. Der Datensatz enthält Merkmale wie Medianalter des Hauses, Anzahl der Zimmer und Einkommen der Nachbarschaft, um den Medianhauspreis (kontinuierlich) vorherzusagen. Wir integrieren CUDA-Unterstützung für GPU-Beschleunigung, falls verfügbar, und visualisieren die Trainings- und Validierungsverluste. Am Ende evaluieren wir das Modell mit Metriken wie Mean Squared Error (MSE), Mean Absolute Error (MAE) und R² Score und erstellen einen Scatter-Plot der vorhergesagten vs. tatsächlichen Werte.\n",
    "\n",
    "Das Notebook ist in folgende Abschnitte gegliedert:\n",
    "1. Datenvorbereitung: Laden und Skalieren des Datensatzes.\n",
    "2. PyTorch Datasets und Dataloaders: Daten für PyTorch vorbereiten.\n",
    "3. MLP-Modell: Definition eines einfachen MLP für Regression.\n",
    "4. Training: Training mit MSE Loss und Visualisierung der Verluste.\n",
    "5. Evaluation: Berechnung von Metriken und Visualisierung der Ergebnisse."
   ],
   "id": "1155ca3c6db98dc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Datenvorbereitung\n",
    "\n",
    "Wir laden den California Housing Dataset und skalieren die Daten, um die Modellleistung zu optimieren.\n",
    "\n",
    "## 1.1 Datensatz laden"
   ],
   "id": "a0cbbbc6ddac2f57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "# Datensatz laden\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target  # Zielvariable: Medianhauspreis in 100.000 USD\n",
    "\n",
    "# Erste Zeilen anzeigen\n",
    "print(X.head())\n",
    "print(f\"Zielvariable (erste 5 Werte): {y[:5]}\")"
   ],
   "id": "fba9ecbf5641d417",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 Daten aufteilen\n",
    "\n",
    "Wir teilen die Daten in Trainings- (70%), Validierungs- (15%) und Testsets (15%)."
   ],
   "id": "9f409edd4c319525"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Aufteilung in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Aufteilung des Trainingsdatensatzes in Trainings- und Validierungsdaten\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1765, random_state=42)  # 0.1765 * 85% ≈ 15%"
   ],
   "id": "bf56955ae7d50a39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.3 Daten skalieren\n",
    "\n",
    "Wir standardisieren die Merkmale und die Zielvariable mit StandardScaler."
   ],
   "id": "9e5bbfcac8c55b59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Skalierung der Merkmale\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_val = scaler_X.transform(X_val)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "# Skalierung der Zielvariable\n",
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_val = scaler_y.transform(y_val.reshape(-1, 1)).flatten()\n",
    "y_test = scaler_y.transform(y_test.reshape(-1, 1)).flatten()"
   ],
   "id": "f9c8ffcc71d32e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Erstellen von PyTorch Datasets und Dataloaders\n",
    "\n",
    "Wir wandeln die Daten in PyTorch-Tensoren um und erstellen Dataloaders für effizientes Training.\n",
    "\n",
    "### 2.1 Benutzerdefiniertes Dataset"
   ],
   "id": "91dbf7d60239d9b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HousingDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)  # Float für Regression\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Datasets erstellen\n",
    "train_dataset = HousingDataset(X_train, y_train)\n",
    "val_dataset = HousingDataset(X_val, y_val)\n",
    "test_dataset = HousingDataset(X_test, y_test)"
   ],
   "id": "4ca02dd241791bb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.2 Dataloaders\n",
    "\n",
    "Wir nutzen pin_memory=True für schnellere Datenübertragung zur GPU, falls CUDA verfügbar ist."
   ],
   "id": "830b300edaefaf32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Gerät bestimmen (GPU oder CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Verwendetes Gerät: {device}\")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Dataloaders erstellen\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory= device != \"cpu\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory= device != \"cpu\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory= device != \"cpu\")"
   ],
   "id": "83edc08324b6d4b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Definition des MLP-Modells\n",
    "\n",
    "Unser MLP hat eine Eingabeschicht, eine versteckte Schicht mit ReLU-Aktivierung und eine Ausgabeschicht für Regression."
   ],
   "id": "a9576a211dcd98f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Parameter\n",
    "input_size = X_train.shape[1]  # Anzahl der Features\n",
    "hidden_size = 64\n",
    "output_size = 1  # Regression: ein kontinuierlicher Wert\n",
    "\n",
    "# Modell instanziieren und auf das Gerät verschieben\n",
    "model = MLP(input_size, hidden_size, output_size).to(device)"
   ],
   "id": "fb98190c356c0b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.1 Funktion zum Plotten der Verluste\n",
    "\n",
    "Wir definieren eine Funktion, die die Trainings- und Validierungsverluste über die Epochen hinweg speichert und einen Plot erstellt."
   ],
   "id": "b3a15c4bb8c73b51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(train_losses, val_losses, num_epochs):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Trainings- und Validierungsverluste über Epochen')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "a823125aa08dc8af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2 Trainingsschleife",
   "id": "8e93c4e3b67cd7d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss-Funktion und Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0015)\n",
    "\n",
    "# Trainingsparameter\n",
    "num_epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward Pass und Optimierung\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validierung\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Verluste plotten\n",
    "plot_losses(train_losses, val_losses, num_epochs)"
   ],
   "id": "be9b270f1e152993",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Evaluation des Modells\n",
    "\n",
    "Wir evaluieren das Modell auf dem Testset mit MSE, MAE und R² Score und visualisieren die Ergebnisse.\n",
    "\n",
    "### 5.1 Metriken berechnen"
   ],
   "id": "a9171b9f40421b17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Modell evaluieren\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs).squeeze()\n",
    "        all_preds.extend(outputs.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Zurückskalieren\n",
    "all_preds = scaler_y.inverse_transform(np.array(all_preds).reshape(-1, 1)).flatten()\n",
    "all_labels = scaler_y.inverse_transform(np.array(all_labels).reshape(-1, 1)).flatten()\n",
    "\n",
    "# Metriken berechnen\n",
    "mse = mean_squared_error(all_labels, all_preds)\n",
    "mae = mean_absolute_error(all_labels, all_preds)\n",
    "r2 = r2_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ],
   "id": "942976b4bbd69e6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.2 Scatter-Plot der Ergebnisse",
   "id": "f0cdcf0928f2b78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Scatter-Plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(all_labels, all_preds, alpha=0.5)\n",
    "plt.plot([all_labels.min(), all_labels.max()], [all_labels.min(), all_labels.max()], 'r--', lw=2)\n",
    "plt.xlabel('Tatsächliche Hauspreise (in 100.000 USD)')\n",
    "plt.ylabel('Vorhergesagte Hauspreise (in 100.000 USD)')\n",
    "plt.title('Vorhergesagte vs. Tatsächliche Hauspreise')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "374d473f14f5e5bf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
